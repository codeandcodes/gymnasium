{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/tmp/ipykernel_7467/3743642644.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(state).to(device)\n",
      "  1%|          | 92/10000 [02:47<5:00:37,  1.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# a) Get deterministic action from Actor network\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         a_t = \u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     noise = torch.normal(\u001b[32m0\u001b[39m, current_noise_sigma, size=a_t.shape, device=device) \u001b[38;5;66;03m# noise tensor\u001b[39;00m\n\u001b[32m    184\u001b[39m     a_t = a_t + noise                 \u001b[38;5;66;03m# add noise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mActor.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m     30\u001b[39m     x = torch.tensor(state).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.max_action\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/modules/activation.py:133\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/gymnasium/lib/python3.13/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(state)\n",
    "        action = self.layers(x) * self.max_action\n",
    "        return action\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        Q = self.layers(sa)\n",
    "        return Q\n",
    "\n",
    "def polyak_average(net_target, net_source, tau):\n",
    "    # --- Perform Polyak Averaging Update ---\n",
    "    # print(f\"Performing Polyak update with tau={tau}...\")\n",
    "    with torch.no_grad(): # IMPORTANT: No gradient tracking needed for this update\n",
    "        # Iterate through the state dictionary of the target network\n",
    "        target_state_dict = net_target.state_dict()\n",
    "        source_state_dict = net_source.state_dict()\n",
    "\n",
    "        for key in target_state_dict:\n",
    "            if key not in source_state_dict:\n",
    "                print(f\"Warning: Key {key} not found in source state_dict. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            target_tensor = target_state_dict[key]\n",
    "            source_tensor = source_state_dict[key]\n",
    "\n",
    "            # Apply the Polyak averaging formula IN-PLACE\n",
    "            # target_tensor = tau * source_tensor + (1 - tau) * target_tensor\n",
    "            target_tensor.mul_(1.0 - tau) # Multiply target by (1-tau)\n",
    "            target_tensor.add_(source_tensor * tau) # Add tau * source\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_dim, action_dim):\n",
    "        # Use separate tensors for better slicing and type handling\n",
    "        self.states = torch.zeros((capacity, state_dim), dtype=torch.float32, device='cpu')\n",
    "        self.actions = torch.zeros((capacity, action_dim), dtype=torch.float32, device='cpu')\n",
    "        self.rewards = torch.zeros((capacity, 1), dtype=torch.float32, device='cpu')\n",
    "        self.next_states = torch.zeros((capacity, state_dim), dtype=torch.float32, device='cpu')\n",
    "        self.dones = torch.zeros((capacity, 1), dtype=torch.float32, device='cpu') # Use float for (1-dones) multiplier\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0 # Pointer to current insertion index\n",
    "        self.size = 0 # Current number of samples in buffer\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to tensors if they aren't already\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device='cpu')\n",
    "        action = torch.as_tensor(action, dtype=torch.float32, device='cpu')\n",
    "        reward = torch.as_tensor([reward], dtype=torch.float32, device='cpu')\n",
    "        next_state = torch.as_tensor(next_state, dtype=torch.float32, device='cpu')\n",
    "        done = torch.as_tensor([done], dtype=torch.float32, device='cpu') # Store 0.0 or 1.0\n",
    "\n",
    "        # Insert data at the current pointer position\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        # Update pointer and size\n",
    "        self.ptr = (self.ptr + 1) % self.capacity # Rolling buffer index\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Generate random indices, ensuring we don't sample beyond current size\n",
    "        random_indices = torch.randint(0, self.size, (batch_size,), device='cpu')\n",
    "\n",
    "        # Sample using the random indices\n",
    "        states = self.states[random_indices]\n",
    "        actions = self.actions[random_indices]\n",
    "        rewards = self.rewards[random_indices]\n",
    "        next_states = self.next_states[random_indices]\n",
    "        dones = self.dones[random_indices]\n",
    "\n",
    "        # Return tuple of tensors, move to correct device\n",
    "        return (\n",
    "            states.to(device),\n",
    "            actions.to(device),\n",
    "            rewards.to(device),\n",
    "            next_states.to(device),\n",
    "            dones.to(device)\n",
    "        )\n",
    "    \n",
    "# constants\n",
    "num_episodes = 10000\n",
    "training_period = num_episodes / 10\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "env = RecordVideo(env, video_folder=\"mountaincar-continuous-agent\", name_prefix=\"training\",\n",
    "                  episode_trigger=lambda x: x % training_period == 0)\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_episodes)\n",
    "state_dim = 2\n",
    "action_dim = 1\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# actor hyperparameters\n",
    "alpha_actor = 0.1\n",
    "\n",
    "# critic hyperparameters\n",
    "alpha_critic = 0.1\n",
    "\n",
    "gamma = 0.999\n",
    "batch_size = 512\n",
    "tau = 0.005 # this is the target network update rate\n",
    "episodes_actor = 10000\n",
    "buffer_size = 1000000\n",
    "\n",
    "initial_noise_sigma = 0.1\n",
    "noise_sigma_min = 0.01\n",
    "noise_sigma_decay_episodes = int(num_episodes * 0.7) # Example decay period\n",
    "noise_sigma_decay = (initial_noise_sigma - noise_sigma_min) / noise_sigma_decay_episodes if noise_sigma_decay_episodes > 0 else 0\n",
    "current_noise_sigma = initial_noise_sigma\n",
    "\n",
    "# initialize networks\n",
    "actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "critic = Critic(state_dim, action_dim).to(device)\n",
    "actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "critic_target = Critic(state_dim, action_dim).to(device)\n",
    "# Set target networks to evaluation mode\n",
    "actor_target.eval()\n",
    "critic_target.eval()\n",
    "loss_critic = nn.MSELoss()\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=alpha_actor)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=alpha_critic)\n",
    "replay_buffer = ReplayBuffer(buffer_size, state_dim, action_dim)\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "all_episode_rewards = []\n",
    "moving_avg_window = 100\n",
    "update_plot_every = 100\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    # print(f\"espisode {episode}\")\n",
    "    \n",
    "    # 1) reset environment\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # a) Get deterministic action from Actor network\n",
    "        with torch.no_grad():\n",
    "            a_t = actor(obs)\n",
    "        noise = torch.normal(0, current_noise_sigma, size=a_t.shape, device=device) # noise tensor\n",
    "        a_t = a_t + noise                 # add noise\n",
    "        a_t = torch.clamp(a_t, -max_action, max_action) # clip a\n",
    "\n",
    "        # b) execute action\n",
    "        a_np = a_t.cpu().numpy()\n",
    "        next_obs, reward, terminated, truncated, info = env.step(a_np)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # c) store transition\n",
    "        replay_buffer.add(obs, a_np, reward, next_obs, done)\n",
    "    \n",
    "        # d) update current state\n",
    "        obs = next_obs\n",
    "\n",
    "        # e) sample and learn if the buffer is large enough\n",
    "        if replay_buffer.size > (1*batch_size):\n",
    "            # i) sample N random transitions (mini-batch)\n",
    "            sb,ab,rb,sb_p,doneb = replay_buffer.sample(batch_size) # (batch_size, 1) tensors\n",
    "            # ii) calculate critic targets using target network\n",
    "            with torch.no_grad():\n",
    "                ab_next_t = actor_target(sb_p)\n",
    "                targetb_q = critic_target(sb_p, ab_next_t)\n",
    "                y = rb + gamma * (1.0-doneb) * targetb_q # calculate all in atch\n",
    "\n",
    "            # iii) update critic network using MSE\n",
    "            current_q = critic(sb,ab)\n",
    "            critic_loss = F.mse_loss(current_q, y)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # iv) update actor network\n",
    "            # Freeze critic parameters during actor update\n",
    "            for p in critic.parameters():\n",
    "                p.requires_grad = False\n",
    "            ab_t = actor(sb)\n",
    "            qb_t = critic(sb,ab_t)\n",
    "            actor_loss = -qb_t.mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Unfreeze critic parameters\n",
    "            for p in critic.parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "            # v) perform soft updates on both target networks\n",
    "            polyak_average(critic_target, critic, tau)\n",
    "            polyak_average(actor_target, actor, tau)\n",
    "        \n",
    "    # Record final episode reward from wrapper info if available\n",
    "    if 'episode' in info:\n",
    "        final_episode_return = info['episode']['r']\n",
    "        all_episode_rewards.append(final_episode_return)\n",
    "\n",
    "    # episode done\n",
    "    current_noise_sigma = max(noise_sigma_min, current_noise_sigma - noise_sigma_decay) # Linear decay example\n",
    "\n",
    "    # --- Update Plot Periodically ---\n",
    "    if (episode + 1) % update_plot_every == 0 and len(all_episode_rewards) > 0:\n",
    "        clear_output(wait=True) # Clear previous output\n",
    "        ax.cla() # Clear axes for fresh plot\n",
    "\n",
    "        # Replot data\n",
    "        ax.plot(all_episode_rewards, label='Episode Reward', alpha=0.6)\n",
    "        if len(all_episode_rewards) >= moving_avg_window:\n",
    "            moving_avg = np.convolve(all_episode_rewards, np.ones(moving_avg_window)/moving_avg_window, mode='valid')\n",
    "            ax.plot(np.arange(moving_avg_window - 1, len(all_episode_rewards)), moving_avg, label=f'{moving_avg_window}-Ep Moving Avg', color='red')\n",
    "\n",
    "        # Reset labels and title\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Total Reward\")\n",
    "        ax.set_title(f\"Training Progress - Episode {episode + 1}\")\n",
    "        ax.legend()\n",
    "\n",
    "        # Display the updated figure in the cell output\n",
    "        display(fig)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
